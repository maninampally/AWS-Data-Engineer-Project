# ğŸš€ Store Ops Analytics â€” AWS Data Engineering Project

A complete **end-to-end data engineering pipeline** built using:

- **AWS S3 Lakehouse (Bronze â†’ Silver â†’ Gold)**
- **PySpark transformations**
- **Great Expectations for data quality**
- **Snowflake as the warehouse**
- **Power BI dashboards**
- **Apache Airflow for orchestration (Docker)**

This project replicates how modern data engineering teams build production data pipelines.

---

## ğŸ— Architecture Overview
```
          BestBuy API  
              â¬‡  
  Bronze Layer â€” Raw JSON in S3  
               â¬‡  
Silver Layer â€” Cleaned PySpark Parquet  
              â¬‡  
  Gold Layer â€” Business KPI Tables  
               â¬‡  
   Snowflake â€” Fact & KPI Models  
              â¬‡  
    Power BI Dashboards  
             â¬‡  
  Airflow â€” Orchestration
```

---

## ğŸ›  Tech Stack

| Layer | Tools |
|------|-------|
| Cloud | AWS S3, SNS, IAM, Secrets Manager |
| Compute | PySpark, Python |
| Data Quality | Great Expectations |
| Warehouse | Snowflake |
| Orchestration | Apache Airflow (Docker) |
| BI | Power BI |
| Storage | Parquet, Iceberg |
| CI/CD | GitHub Actions |

---

---

## ğŸ”¥ Key Features

### âœ” 1. BestBuy API Ingestion (Bronze)
- Secure API key via **AWS Secrets Manager**
- Handles pagination & request limits
- Stores raw JSON in S3 (`dt=YYYY-MM-DD`)
- SNS alerts for success/failure

### âœ” 2. PySpark Transformations (Silver)
- Data cleaning and schema enforcement  
- Category hierarchy extraction  
- Price validation  
- Synthetic **inventory** & **sales** simulation  
- Saves optimized Parquet to S3  

### âœ” 3. Business KPIs (Gold)
- Product pricing KPIs  
- Sales KPIs (revenue, units sold)  
- Inventory KPIs (stock, low-stock detection)  
- Clean output for analytics + Snowflake  

### âœ” 4. Data Quality (Great Expectations)
- Null checks, type checks, value rules  
- Price consistency validation  
- KPI rule validation  
- Pipeline stops if validation fails  

### âœ” 5. Snowflake Warehouse
- Loads Gold Parquet â†’ Stage  
- `COPY INTO` â†’ Staging  
- `MERGE INTO` â†’ Fact tables  
- Fully incremental & idempotent  

### âœ” 6. Power BI Dashboards
- Product performance  
- Sales trends  
- Inventory health  
- Category insights  

### âœ” 7. Airflow Orchestration (Docker)
- DAG: `bronze â†’ silver â†’ gold â†’ snowflake`
- Retries, logging, scheduling  
- Runs via Docker Compose  

---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ bronze/ # Ingestion scripts
â”‚ â”œâ”€â”€ silver/ # Transformations
â”‚ â”œâ”€â”€ gold/ # KPI calculations
â”‚ â”œâ”€â”€ snowflake_load/ # Warehouse loader
â”‚ â””â”€â”€ utils/ # Helpers & validation
â”‚
â”œâ”€â”€ expectations/ # Great Expectations config & suites
â”‚ â”œâ”€â”€ ge_config.yml
â”‚ â””â”€â”€ suites/
â”‚ â”œâ”€â”€ silver_products_suite.yml
â”‚ â”œâ”€â”€ silver_inventory_suite.yml
â”‚ â”œâ”€â”€ silver_sales_suite.yml
â”‚ â””â”€â”€ gold_kpis_suite.yml
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ bestbuy_ingest_to_gold_dag.py
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ callbacks.py
â”‚ â””â”€â”€ variables.py
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ iceberg/ # Iceberg DDL
â”‚ â””â”€â”€ snowflake/ # Snowflake DDL + MERGE logic
â”‚
â”œâ”€â”€ bi/
â”‚ â””â”€â”€ powerbi/ # Dashboard notes
â”‚
â””â”€â”€ env/
â”œâ”€â”€ dev.yaml
â””â”€â”€ prod.yaml
```

---

## ğŸš€ Running the Pipeline
# ğŸƒâ€â™‚ï¸ How to Run the Pipeline (From Cloning the Repository)

Follow these steps to run the entire data engineering pipeline from scratch.

---

## 1ï¸âƒ£ Clone the Repository
```
git clone https://github.com/maninampally/store-ops-analytics.git
cd store-ops-analytics
```
---

## 2ï¸âƒ£ Create & Activate Virtual Environment
```
### Windows
python -m venv .venv
.venv\Scripts\activate
```

### Mac / Linux
```
python3 -m venv .venv
source .venv/bin/activate
```

---

## 3ï¸âƒ£ Install All Dependencies
```
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Configure AWS Credentials

You must have:
- AWS Access Key ID  
- AWS Secret Access Key  
- Region (ex: us-east-2)
```
aws configure
```

This allows:
- Bronze job to write to S3  
- Secrets Manager access  
- SNS notifications  

---

## 5ï¸âƒ£ Add Your BestBuy API Key to AWS Secrets Manager

Create a secret:

aws secretsmanager create-secret
--name bestbuy_api_key
--secret-string "YOUR_API_KEY"


Your code automatically retrieves it using:
```
get_bestbuy_api_key()
```

---

## 6ï¸âƒ£ Run the Full ETL Pipeline (Step-by-Step)

### ğŸ‘‰ Bronze Layer (Raw Data Ingestion)
Fetch BestBuy product data and store JSON in S3:
```
python -m etl.bronze.bestbuy_pull
```

### ğŸ‘‰ Silver Layer (Clean Transformations)
Run PySpark cleaning + inventory + sales simulation:
```
python -m etl.silver.curate_catalog
```


### ğŸ‘‰ Gold Layer (Business KPIs)
Generate aggregated KPI tables:
```
python -m etl.gold.marts_store_ops
```


### ğŸ‘‰ Load Gold Tables into Snowflake
```
python -m etl.snowflake_load.load_gold_to_snowflake
```


---

## 7ï¸âƒ£ Start Apache Airflow (Optional â€“ Full Automation)

Make sure Docker Desktop is running.

Start Airflow:
```
docker compose up --build
```


Airflow UI:

http://localhost:8080


Trigger the DAG:
```
bestbuy_ingest_to_gold_dag
```

This runs:

- Bronze ingestion  
- Silver transformation  
- Gold KPI generation  
- Snowflake load  
- Notifications  

ALL automatically.

---

## 8ï¸âƒ£ View Your Data in Snowflake & Power BI

### Snowflake
Use:
```
USE SCHEMA store_ops.gold;
SELECT * FROM gold_sales_kpis;
```

### Power BI
1. Open Power BI Desktop  
2. Connect â†’ â€œSnowflakeâ€  
3. Load gold tables  
4. Build dashboards  

---

## ğŸ‰ Pipeline Complete!

You now have:

- Automated ingestion  
- Clean curated datasets  
- Validated KPIs  
- Snowflake models  
- BI dashboards  
- Airflow orchestration  

End-to-end production-style data engineering pipeline!<img width="1404" height="12305" alt="image" src="https://github.com/user-attachments/assets/b3562653-efd7-4914-9674-75ed31712f57" />
